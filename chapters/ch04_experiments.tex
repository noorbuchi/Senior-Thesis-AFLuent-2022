\chapter{Experimental Results}
\label{ch:experiments}

\section{Experimental Design}
\label{sec:expiremental_design}

Following the completion of AFLuent, an evaluation step is needed to understand
how accurate the tool is in localizing faults. This section discusses the design
of this experiment including early steps in choosing and filtering a sample to
test AFLuent on.

\subsection{Approach Overview}
\label{subsec:approach_overview}

In order to evaluate AFLuent, several prerequisites are needed that enable
collecting data for analysis. The primary requirement is a collection of Python
program, which are susceptible of becoming faulty. Additionally, this
collection's complexity must be comparable to code typically written by novice
developers, which AFLuent targets. Another crucial requirement before evaluation
can begin is a test suite for the selected code python code. The test suite must
be executable by Pytest and covers as much as possible of the code under test to
maximize the ability to localize faults. Assuming that these prerequisites are
met and are available, the evaluation of AFLuent can then be generally described
by these four main steps:

\begin{enumerate}
    \item Introduce a bug/fault in the code under test
    \item Run the test suite with AFLuent
    \item Produce a report of suspicious statements as ranked by AFLuent
    \item Asses how far the in the produced ranking report was the location of
    the introduced bug
\end{enumerate}

While these steps are great for planning and describing the intention in evaluating
AFLuent, they fall short of providing a clear and detailed plan. The sections
below will expand on the points discussed here, elaborate on how the
prerequisites are met, and describe the systematic and automated approach in evaluation.

\subsection{Research Questions}
\label{subsec:research_questions_eval}

Before discussing the evaluation process, it's important to clearly state the
questions to answer. Previously, Section\ref{sec:researchq} brought up few
research question concerning the implementation and evaluation of AFL in Python.
Related Work and Methods section addressed \hyperref[para:RQ1.1]{\emph{RQ1.1}} as well as
\hyperref[para:RQ2]{\emph{RQ2}}, however, the answer \hyperref[para:RQ1.2]{\emph{RQ1.2}} remains unclear. While \hyperref[para:RQ1.2]{\emph{RQ1.2}}
generally involved the efficiency and accuracy of AFLuent, there was no mention
of the metrics or the comparisons needed to achieve an answer. In general, there
are sixteen different approaches that will be evaluated and compared. For every AFL
equation used, Tarantula, Ochiai, Ochiai2, and Dstar, there are four tie breaking
approaches. Each equation-tieberaker pair will be evaluated on the same dataset,
where the resulting rankings will be used to produce a score to assess how
close the produced ranking are to localizing the fault correctly. In addition to
this score, the time taken to run each approach will be recorded to compare
their time overhead.

\subsection{Choosing a Sample}
\label{subsec:choosing_sample}

Since AFLuent is targeted towards novice developers with little experience
programming in Python, the evaluation strategy should reflect that by running
AFLuent on novice written code. However, considering that this study does not
involve human subjects, novice developers cannot be asked to write code for the
purposes of this evaluation. To simulate novice written code as best as
possible, the GitHub repository \code{TheAlgorithms/Python}\cite{the_algorithms_python}
is used as the sample. This educational repository is a collection of projects
that implement several fundamental algorithms in Python that range from ciphers,
sorting, graphs, string operations to financial and physics equations.
It resembles novice written
code because, in a sense, beginner developers write these programs while
learning the basics of programming. However, a problem
with using this code is the lack of unit tests that pytest can run, since most
tests are written using Python's doctest. With that in mind, there are Python
tools such as Pynguin\cite{Lukasczyk_Pynguin_Automated_Unit_2022}
that can be used to automatically generate a test suite for these projects.
With the many limitations in Pynguin and the long waiting times to generate
tests, some filtering of the projects is needed.


\subsubsection{Filtering Sample}
\label{subsubsec:filtering_sample}

There are many issues that could arise when automatically generating a test
suite using Pynguin. To mitigate these problems, the following criteria was used
to eliminate entire projects, delete modules, or trim some code.

\begin{itemize}
    \item Projects that import external modules: In order to facilitate and
    expedite generating a test suite using Pynguin, several projects that use
    external packages were removed. These packages include \code{scikit-learn},
    \code{Tensorflow}, \code{Matplotlib}, \code{Sympy}, and \code{PIL}.
    Generating data and creating unit tests for projects that using theses
    packages can be difficult because they are time consuming or simply cannot
    be tested due to their graphical output.
    \item Functions that do not take input or return no results: some implemented
    functions do not accept arguments as input. This makes them difficult to
    test because no data can be passed to them. Additionally, functions that do
    not return a result are also difficult because there is no output to be
    checked. Functions that are part of an object oriented structure that change
    the state of an object without returning a value were kept since their
    results can be tested. However, other functions such as \code{main()} which
    read input from the console and controlled the flow were removed.
    \item Functions with missing type hints in signature: generating data for
    functions with unknown input type would be challenging for Pynguin to
    perform. Therefore, these functions were removed. In some instances where
    the documentation explicitly stated the type of input for the function, type
    hints were manually added to avoid the removal of the function. This was
    especially frequent in sorting function, in which the input was specified as
    integer values.
    \item Code snippets under \code{if \_\_name\_\_ == "\_\_main\_\_"}: In most
    instances the code under this if statement either ran the doctest tests, or
    called a separately implemented main function. Since there is no use for
    these snippets and to clean up the sample, these snippets were removed.
\end{itemize}


\begin{table}[!htb]
    \centering
	\begin{tabular}{|c|c|c|c|}
        \hline
        arithmetic\_analysis & audio\_filters   & backtracking       & bit\_manipulation    \\
        blockchain           & boolean\_algebra & cellular\_automata & ciphers              \\
        compression          & conversions      & data\_structures   & divide\_and\_conquer \\
        dynamic\_programming & electronics      & financial          & genetic\_algorithm   \\
        geodesy              & graphs           & greedy\_methods    & hashes               \\
        knapsack             & linear\_algebra  & maths              & matrix               \\
        physics              & scheduling       & searches           & sorts                \\
        strings              &                  &                    &\\

        \hline
	\end{tabular}
	\caption{Remaining Projects Prior to Test Generation}
	\label{table:remaining_projects}
\end{table}

Following the initial phase of filtering the codebase, general statistics and
observations were recorded for the remaining sample thus far.
Table\ref{table:remaining_projects} shows the name of the remaining project.
Additionally, SLOCcount was used to calculate the number of non-comment line of
code included in the sample. The output from the tool is shown in
Fig\ref{fig:SLOCcount_phase1}. Overall, there was 12199 lines of code remaining
in the sample prior to automatically generating tests using Pynguin.

\begin{figure}[!htb]
	\begin{center}
		\begin{lstlisting}
            SLOC	Directory	SLOC-by-Language (Sorted)
            2396    data_structures python=2396
            1856    ciphers         python=1856
            1583    maths           python=1583
            900     graphs          python=900
            664     sorts           python=664
            559     strings         python=559
            543     conversions     python=543
            412     linear_algebra  python=412
            365     divide_and_conquer python=365
            360     matrix          python=360
            359     dynamic_programming python=359
            308     backtracking    python=308
            248     compression     python=248
            236     searches        python=236
            200     arithmetic_analysis python=200
            173     hashes          python=173
            171     audio_filters   python=171
            161     bit_manipulation python=161
            118     cellular_automata python=118
            109     boolean_algebra python=109
            97      scheduling      python=97
            94      blockchain      python=94
            86      electronics     python=86
            67      genetic_algorithm python=67
            43      financial       python=43
            40      knapsack        python=40
            37      geodesy         python=37
            11      greedy_methods  python=11
            3       physics         python=3

            Totals grouped by language (dominant language first):
            python:       12199 (100.00%)
        \end{lstlisting}
		\caption{\label{fig:SLOCcount_phase1} SLOCcount Output After Initial Filtering}
	\end{center}
\end{figure}


\subsubsection{Generating a Test Suite}
\label{subsubsec:generating_test_suite}

Once initial filtering of the codebase was completed, Pynguin was ran to
generate tests. However, additional issues came up in this process that required
additional filtering to be done. The new content was filtered as follows:
\begin{itemize}
    \item When attempting to generate tests,
    Pynguin failed in 54 modules and threw error messages indicating possible
    bugs in the tool. To limit the time taken to generate tests, 15 minutes were
    given per module, where no tests were generated for 73 modules due to
    getting timed out. Finally, there were 249 modules where tests were
    successfully generated. Modules with failed and timed out runs were removed
    due to the lack of tests that cover them.
    \item Some generated tests were faulty and caused errors when ran, or
    indeterminately failed making them flaky. Those tests were removed in some
    instances or fixed when possible.
    \item Since AFLuent relies on a thorough test suite with high coverage in
    order to find bugs accurately, modules with below 85\% coverage were removed.
\end{itemize}

\begin{figure}[!htb]
	\begin{center}
		\begin{lstlisting}
            SLOC	Directory	SLOC-by-Language (Sorted)
            934     maths           python=934
            826     ciphers         python=826
            576     data_structures python=576
            496     sorts           python=496
            465     conversions     python=465
            454     graphs          python=454
            356     strings         python=356
            217     dynamic_programming python=217
            192     backtracking    python=192
            173     hashes          python=173
            133     bit_manipulation python=133
            101     searches        python=101
            90      linear_algebra  python=90
            69      electronics     python=69
            61      divide_and_conquer python=61
            43      financial       python=43
            42      scheduling      python=42
            37      geodesy         python=37
            27      knapsack        python=27
            16      arithmetic_analysis python=16
            14      cellular_automata python=14
            11      greedy_methods  python=11
            10      compression     python=10
            3       physics         python=3

            Totals grouped by language (dominant language first):
            python:        5346 (100.00%)

        \end{lstlisting}
		\caption{\label{fig:SLOCcount_phase2} SLOCcount Output After Initial Filtering}
	\end{center}
\end{figure}

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=15.5cm]{cyclomatic_complexity.png}
		\caption{\label{fig:cyclomatic_complexity_of_sample} Cyclomatic
		Complextiy of Sample}
	\end{center}
\end{figure}

After the second filtering step SLOCcount statistics were collected again to get
information on the sample, the output from the tool is shown in
Figure\ref{fig:SLOCcount_phase2}. While the codebase was significantly reduced in
size, the resulting test suite contains 1105 test cases with 99\% coverage.
In order to get a better understanding of the remaining sample, additional data
was collected on the cyclomatic complexity the functions in the code. This
information would give some ideas on the structure of the code regarding if
statements, loops and other constructs. The cyclomatic complexity of the
remaining 516 functions was calculated and plotted as shown on Figure
\ref{fig:cyclomatic_complexity_of_sample}. The figure shows a box and whiskers
plot of this data, where the lower quartile and the minimum are equal causing a
lower whisker of zero length. Additionally, the plot shows a median cyclomatic
complexity score of 3 and a maximum of 8. Lastly, few data outliers go beyond
and up to a score of 24. These results mean that the code is very linear, they also suggest that it resembles
what novice developers write as simple only block function and minimal branching.

\section{Evaluation}
\label{sec:evaluation}

\subsection{Data Collection}
\label{subsec:data_collection}

After choosing and filtering the sample, the evaluation of AFLuent would be
conducted by repeating three main steps: (1) Introduce a single bug in the codebase,
(2) Run AFLuent expecting the tests to fail, and (3) collect the reports
produced by AFLuent for analysis later on. With the large sample and the
possibility of inserting bugs in many possible locations, there was a need for
an automated approach to collect this data. Some existing tools such as
\emph{mutmut}\cite{mutmut} already perform similar steps for
mutation testing and it could be easily repurposed to generate the bugs/mutants and run
AFLuent after inserting a mutant into the codebase. Futhermore, \emph{mutmut}
supports a hook function that facilitates collecting results after each run and
before the next mutant is applied. Lastly the test suite run command can be
modified to run AFLuent in evaluation mode and collect fault localization data.
Overall, for each bug/mutant, the following data would be collected to use for
evaluation:
\begin{itemize}
    \item Statement rankings produced by AFLuent for each 16
    approach-tiebreaker combination
    \item Per-test coverage report
    \item Timing report containing the time taken to run the test suite befor
    fault localization and the time to perform fault localization and get
    rankings.
    \item Information about the mutant such as the file and line number it was
    added to as well as the mutated code.
\end{itemize}

AFLuent ranking and timing data was collected on five different machines each with
Ubuntu operating system, \code{Intel Xeon E3-12xx} CPU, and 2GB of memory. Docker containers were created
on each machine, where Python 3.10.2 was installed along with AFLuent and
\code{mutmut}. After running \emph{mutmut} on the filtered algorithms, it was able
to generate 7767 mutants in which 4123 were killed, 3490 survived, and 154 were
timed out by \code{mutmut}. The mutants were classified using the following criteria:

\begin{itemize}
    \item Killed mutants: cases where the mutated code caused a failure in the
    test suite, as expected, prompting AFLuent to localize the fault and
    generate ranking reports.
    Every mutant that generated the ranking reports is
    considered killed.
    \item Survived: cases where the mutation did not cause a failure in the test
    suite. AFLuent was not run in these cases since no test case failed. Runs
    that generated per-test coverage and timing reports but no ranking reports
    are for mutants that survived.
    However, per-test coverage and timing reports were generated.
    \item Timed out: special cases where \emph{mutmut} reported to run a mutant
    but no report was produced by AFLuent. The log produced by \code{mutmut}
    confirms that these mutants were timed out after taking too long to complete
    the test run.
\end{itemize}

When evaluating AFLuent's effectiveness in ranking lines of code, only the
results from killed mutants are relevant since that's when reports were
generated. As for efficiency evaluation, data can be split up to two categories,
time taken by AFLuent to execute tests while calculating per-test coverage, and
time taken to produce rankings to find faults. Both types of mutants, killed and
survived, produced data points on the time taken to run the tests. However, only
killed mutants can produce data points on the time taken to perform
localization. Therefore, a different number of data points will be used for each
category.

\subsection{Results}
\label{subsec:eval_results}

The collected data is visualized in this subsection to compare the different
equations to each other, additionally, tie breaking techniques are compared to
determine the most effective. In order to quantify the effectiveness of each
approach, an \emph{EXAM} %TODO: add citation
score is calculated for each report produced from a killed mutant. This score
represents the percentage of reported statements that a developer must parse
through before reaching the correct line where the bug exists. To produce this
score, the reported results were filtered by project so that the reported
results only include lines from that project. For example, when the \emph{EXAM}
score is calculated for a fault in the \code{bit\_manipulation} project, the
percentage of lines belonging to that project that a developer must read before
finding the fault is the score. To visualize the data, box and whisker plots are
used to show the scores of each equation-tiebreaker combination. This form of
visualizing the data allows each point to be represented in the graph and gives
a clear indicator of where outliers are.

\subsubsection{Comparison by Equation}
\label{subsubsec:comparison_by_equation}

\subsubsection{Comparison by Tiebreaker}
\label{subsubsec:comparison_by_tiebreaker}

\subsubsection{Time Efficiency}
\label{subsubsec:time_Efficiency}

In order to provide a time comparison that accounts for the different cases of
AFLuent runs, three different categories of time data are included: (I) Time to
execute tests without AFLuent, (II) Time to execute tests with AFLuent enabled,
(III) Time to locate faults and perform tie breaking using all
equation-tiebreaker combinations. Generally all these times were calculated when
all 1105 test cases in the project's suite were ran.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=15.5cm]{test_timings.png}
		\caption{\label{fig:test_timings} Test Execution Time}
	\end{center}
\end{figure}

Figure \ref{fig:test_timings} compares the time taken to execute all test
functions with and without using AFLuent. The baseline includes
4000 data points where the tests were ran without AFLuent. On the other hand
7613 data points are plotted for when AFLuent generated a timing report. The
figure shows a clear difference between the two boxplots, where the median run with
AFLuent took longer time to complete than the median run for the baseline.
Additionally, the long upper whisker indicates that
including AFLuent increases the chances that the tests could take substantially
longer to run. This increase is somewhat expected since AFLuent introduces
wrapper functions around each test execution to calculate per-test coverage. And
while this increase is quite small,considering the large size of the test suite,
minimizing the additional time cannot be done through AFLuent since the tool
relies on Coverage.py to calculate per-test coverage. After performing the The
Wilcoxon Rank-Sum Test on both sets, the resulting p-value of
\code{8.035528e-157} confirms that there is statistically significant difference
between the two sets.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=15.5cm]{localization_timings.png}
		\caption{\label{fig:localization_timings} Fault Localization Time}
	\end{center}
\end{figure}

Another useful time metric for developers hoping to use AFLuent is the time the
took takes to locate faults. And while
comprehensive time data for each equation and tie breaking approach was not
collected, some conclusions can be drawn from the most time consuming case.
Figure \ref{fig:localization_timings} shows the time taken to generate a
statement ranking following a test failure. This includes the time to calculate
suspiciousness scores using all equations and to retrieve and evaluate tie
breaking values using all approaches. In general, this is the worst possible
scenario for AFLuent localization time. While the median time is quite large,
around 115 second, so is the test suite. For every run, AFLuent generates and parses
abstract syntax trees using libcst and calculates cyclomatic complexity fo all
files covered in the suite. In the case of this evaluation, this includes all
files in the codebase. Further discussion on how this time can be minimize is
found in Section \ref{sec:future_work}: Future Work.

\section{Threats to Validity}