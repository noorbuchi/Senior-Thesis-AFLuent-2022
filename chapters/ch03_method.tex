\chapter{Method of Approach}
\label{ch:method}

This chapter describes the implementation of AFLuent and the experiment setup and
execution process. More specifically, the reasoning behind design decisions and
the result are the main focus. Additionally, charts and diagrams  are used to
demonstrate the the algorithms, structure, and flow of execution.

\section{Development Environment and Toolset}
\label{sec:DevEnviron}

Prior to explain how implementation was completed, a ground-up overview of the
tools used and their roles is necessary to establish definitions and facilitate
the understanding of how dependencies they are connected. AFLuent being a Python
tool allows its development to rely on a wide variety of helpful and popular
tools. Some of the most important tools and dependencies are discussed below.

\subsection{Poetry}
\label{subsec:poetry}

Poetry is a Python virtual environment management tool that allows developers to
set up an isolated environment for their projects. Furthermore, it manges the
installation of Python dependencies on the virtualenv and updates them when
necessary. Poetry has a crucial role in the implementation of AFLuent since it's
used to make the development process simpler, its role also goes beyond
that to the packaging and publishing of AFLuent to the Python Package Index
(PyPI). Poetry's ability to abstract and simplify all the details in creating a
Python package, makes it essential for development. However, from a user's
perspective, Poetry is not necessary to run or use the tool for fault localization.

\subsection{Pytest}
\label{subsec:pytest}

Pytest is a Python testing framework used to write and execute a variety of
tests, especially unit tests. Many developers in the community have contributed
to Pytest through plugins that extend Pytest's functionality and allow it to
accomplish new beneficial tasks. As mentioned previously, AFLuent is a Pytest
plugin that adds automated fault localization features. By definition, AFLuent
relies on Pytest in order to function properly. More details on how AFLuet is
integrated with pytest can be found in Section \ref{sec:makingPytestPlugin}.

\subsection{Coverage.py}
\label{subsec:coverage}

Spectrum based fault localization requires data on code coverage and test
results in order to calculate and rank suspicious of elements in the code.
Coverage.py\cite{coverage_py_website} is a Python tool that provides an easy to use application
programming interface to collect that data. The tool also provides various
configuration for the user to skip certain file or directories from being
considered. AFLuent relies on this tool to calculate what's known as per-test
coverage. This data describes the lines of code covered by a single test case
and organized in an accessible way to find out the number of passing and failing
test cases that executed each line. Automated fault localization approaches
require this information in order to calculate suspiciousness scores and
Coverage.py can provide that relatively easily.

\subsection{Radon}
\label{subsec:radon}

Radon is a Python tool used to calculate metrics of code complexity.
Specifically, it's used in AFLuent to calculate cyclomatic complexity of some
elements to break ties in rankings. When the user requests that AFLuent
considers cyclomatic complexity to be considered when ranking lines, Radon is
used to create a dataset of complexity scores. Since single lines cannot have a
cyclomatic complexity score, they are assigned the score of the function they're
a part of. Furthermore, the dataset is organized to facilitate efficient
searching by the line number in question.

\subsection{Libcst}
\label{subsec:libcst}

In order to provide additional methods to breaking ties between element
rankings, Libcst is used to create and abstract syntax tree of the code in
question. This approach allows AFLuent to assess code complexity using the
code's syntax. Additional details on the use of Libcst are discussed in section \ref{sec:complexity}:
Complexity and Tie Breaking.

\section{AFLuent as a Pytest Plugin}
\label{sec:makingPytestPlugin}

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=11.9cm]{pytest_flowfchart.png}
		\caption{\label{fig:pytest_flow} Pytest Tasks Flowchart}
	\end{center}
\end{figure}

Pytest provides simple and intuitive ways of extending its functionality through
hooks. These hooks break up the standard Pytest execution steps and allow
external code to run at these points. A plugin is essentially a collection of
packaged hooks that fit in the workflow of Pytest. AFLuent makes use of five
different hooks to implement automated fault localization.
Fig.\ref{fig:pytest_flow} shows a general overview of the steps changed in the
workflow of Pytest. Additionally, the section below describes in details how each
step was modified.

\section{Installing AFLuent}
\label{subsec:afluent_install}

As part of the goal of creating a novice friendly tool, it's very important that
AFLuent can be installed and set up easily on any system with little to no
complications. To achieve that, AFLuent is published to the Python Package Index
(PyPI), which makes it installable through the \code{pip install afluent}
command. Once this command runs successfully, AFLuent is automatically
integrated with Pytest as a plugin and will be ran with every pytest session
unless otherwise disabled intentionally. AFLuent's dependency on Coverage.py
creates a small but avoidable conflict. In the case that AFluent and another plugin that
utilizes Coverage.py is active in the same Pytest session, various errors might
occur where coverage information is not calculated correctly, thus affecting the
automated fault localization process. This issue was prevalent when specifically
using \code{pytest-cov} plugin, which produces a coverage report at the end of a
Pytest session. AFLuent handles this problem by checking for the presence of
other Coverage.py reliant plugins and warns the user. Due to this issue,
AFLuent will not run in the session unless specified specifically using the
command-line arguments discussed in the following section.

\subsection{Adding Command-Line Arguments}
\label{subsec:pytest_cli}

Pytest already supports a multitude of command line arguments that allow the
user to pass configuration that change how the test suite is executed and
reported. Similarly, AFLuent requires user passed arguments to complete a
variety of tasks. The hook \code{pytest\_addoption} allows adding new
arguments in a fashion similar to the \code{argparse} Python library.
The \code{argparse} library supports features that allow easy parsing and validation of
command-line arguments. Furthermore, it facilitates working with input with
various data types.
The added arguments accept values that specify the following:
\begin{itemize}
    \item If AFLuent should be enabled for the current Pytest session.
    \begin{itemize}
        \item Flags: \code{---afl-debug} or \code{---afl}
    \end{itemize}
    \item The techniques for suspiciousness score calculations
%    TODO: add approaches to the list after implementation
    \begin{itemize}
        \item \code{---tarantula}: enable fault localization using Tarantula approach
        \item \code{---ochiai}: enable fault localization using Ochiai approach
        \item \code{---dstar}: enable fault localization using DStar approach
        \item \code{---dstar-pow}: used to set the power to use in the DStar
        approach. Defaults to 3
        \item Multiple approaches can be used at the same time, however, results
        will be sorted by the approach passed first.
    \end{itemize}
    \item The number of results to display in the report
    \begin{itemize}
        \item \code{---afl-results}: number of top results to show in the
        terminal window. Defaults to 20
    \end{itemize}
    \item The directories and files to be ignored while calculating coverage
    \begin{itemize}
        \item \code{---afl-ignore}: Used to pass directories and files to
        ignore. Example: \code{tests/*} will ignore all files and directories
        inside the \code{tests} folder.
    \end{itemize}
    \item The types of file reports to created after the Pytest session is over
    \begin{itemize}
        \item \code{---report}: accepts \code{json} or \code{csv} and generate
        reports with the passed format.
        \item \code{---per-test-report}: requires that a per-test coverage
        report is produced. This report is only generated in JSON format.
    \end{itemize}
    \item If complexity metrics should be used in breaking ties of ranked elements
    \begin{itemize}
        \item \code{---cyclomatic-complexity}: use cyclomatic complexity metrics
        in breaking ties found in suspiciousness scores
        \item \code{---syntax-complexity}: use syntax complexity metrics
        in breaking ties found in suspiciousness scores
    \end{itemize}
\end{itemize}

\subsection{Activating AFLuent}
\label{subsec:activate_afluent}

After arguments are passed, the next steps parses through some of them to check
if AFLuent was enabled and to validate some of their values. The Pytest hook
\code{pytest\_cmdline\_main} gives access to the collected configuration. In
this hook, checks are conducted to see if there are other active plugins that
utilize Coverage.py. These checks are necessary due to conflicts in collecting
data when multiple plugins use Coverage.py simultaneously. The problem is
resolved through warnings to the user that ask to disable these plugins in order
for AFLuent to run properly. Lastly, if AFLuent is enabled, an \code{Afluent} object is initialized
with the passed configuration and registered as a plugin for the current Pytest session.

\subsection{Calculating Per-test Coverage and Test Result}
\label{subsec:calculate_coverage}

Per-test coverage is defined here as the collection of lines executed by
each individual test case. In order to collect this data, AFLuent adds a wrapper
that executes code before and after each test case function is called. The
\code{pytest\_pyfunc\_call} hook is used in this scenario. The execution steps
are as follows:

\begin{enumerate}
    \item \code{cov.start()} begins recording coverage
    \item the hook yields back control to Pytest which calls the individual test case
    \item \code{cov.stop()} stops recording coverage
    \item The collected data is then organized in a simpler structure defined as
    the program spectra
    \item The process repeats for every test case
\end{enumerate}

Once per-test coverage is completed, the \code{pytest\_runtest\_makereport} hook
is used to get the outcome of each test case and add it to the program spectra
structure. There are three possible test outcome in Pytest, `Passed', `Failed',
and `Skipped'.

\subsection{Reporting Results}
\label{subsec:report_results}

The last step in AFLuent execution as part of Pytest is to report the fault
localization outcome. The \code{pytest\_sessionfinish} hook is used to detect the
exit code of the session and display output on the console accordingly. An exist
code of 0 means that all tests have passed and there is no need to perform fault
localization, therefore, a message would display that to the user before
finishing the Pytest run. On the other hand an exit code of 1, would indicate
that at least one test case failed during the pytest session. In this scenario,
AFLuent initializes a \code{Spectrum} object, which is responsible for parsing
through the program spectra structure, ranking elements based on suspiciousness,
and producing an output to the console.

\section{AFLuent's Components}
\label{sec:components}

Considering all the steps involved in calculating and reporting suspiciousness
of elements, AFLuent is implemented in an object oriented style that focuses on
the readability and maintainability of the code. By splitting up AFLuent to a
collection of objects organized in a hierarchy, the testing approach becomes more
clear and debugging becomes easier. This section discusses the objects oriented
structure starting with the least complex.

\subsection{Line Object}
\label{subsec:line_obj}

A line is the simplest unit in a large project or program. With that in mind, a
\code{Line} object describes the attributes and implements the functionalities
associated with a single line in the context of automated fault localization.
Important attributes of a line include identifiers such as the line number and
file path it belongs to. Other line-specific information include a list of
failed test cases that executed this line, and another for tests that passed.
This information is crucial in calculating the one of the main focuses of
AFLuent, which is suspiciousness scores. These scores are also stored as
attributes of each Line object. Another stored attribute is complexity,
which is a metric associated with each line and used to break ties in rankings.
Further reading on complexity metrics can be found in section \ref{sec:complexity}.

As for functions, a Line object contains an implementation of the three
equations, Tarantula shown in Fig.\ref{fig:tarantulaEquation}, Ochiai shown in
Fig.\ref{fig:ochiaiEquation}, and DStar shown in Fig.\ref{fig:dstarEquation},
where suspiciousness scores are calculated and rounded up to four decimal places.
The mathematical implementations of these functions also handle and account for
division by zero. Depending on the equation, there could be multiple instances
where division by zero is possible. The following subsections discuss how this
issue was handled for each equation.

% TODO: the number of equations should be updated once new ones are added

\subsubsection{Division by Zero: Tarantula}
\label{subsubsec:div_by_zero_taran}
In the Tarantula equation, division by zero occurs if either the number of
total passing tests or number of total failing tests is zero. In the case that
there are no failing test-cases in the whole test suite, then the suspiciousness
of all covered statements is zero (not suspicious) since there was no fault. The
other edge case occurs when there are no passing tests in the whole test suite,
in which every covered line has the maximum suspiciousness of 1. It is important
to acknowledge that the outcomes reached here only apply to code that has been
covered by the test suite. Faulty lines, which are not covered by any test case
will not be investigated since there is no data to calculate their
suspiciousness. Using values plugged in to Fig.\ref{fig:tarantulaEquation}, the
examples in Fig.\ref{fig:taran_div_by_zero_1} and Fig.\ref{fig:taran_div_by_zero_2} show
the two possible cases where division by zero might occur in the Tarantula
equation.

\begin{figure}[!htb]
	\begin{center}
        When \textbf{N$_{CF}$ = 0} and \textbf{N$_{UF}$ = 0 }
		\begin{equation}
			Tarantula(element) = \frac{\frac{\textbf{0}}{\textbf{0}}}{\frac{\textbf{N$_{CS}$}}{\textbf{N$_{CS}$}+\textbf{N$_{US}$}} + \frac{\textbf{0}}{\textbf{0}}}
		\end{equation}
		\caption{\label{fig:taran_div_by_zero_1} Tarantula Division by Zero Example 1}
	\end{center}
\end{figure}

\begin{figure}[!htb]
	\begin{center}
        When \textbf{N$_{CS}$ = 0} and \textbf{N$_{US}$ = 0 }
		\begin{equation}
			Tarantula(element) = \frac{\frac{\textbf{N$_{CF}$}}{\textbf{N$_{CF}$} + \textbf{N$_{UF}$}}}{\frac{\textbf{0}}{\textbf{0}} + \frac{\textbf{N$_{CF}$}}{\textbf{N$_{CF}$}+\textbf{N$_{UF}$}}}
		\end{equation}
		\caption{\label{fig:taran_div_by_zero_2} Tarantula Division by Zero Example 2}
	\end{center}
\end{figure}

Fig.\ref{fig:taran_div_by_zero_1} shows the case where there is a total of zero
failed test cases, covering AND not covering the element, in which a score
of 0 is assigned to the element. On the other hand,
Fig.\ref{fig:taran_div_by_zero_2} shows the case where there is a total of zero
passing test cases, in which a maximum score of 1 is assigned to the element.

\subsubsection{Division by Zero: Ochiai}
\label{subsubsec:div_by_zero_ochiai}

Division by zero occurs in the Ochiai formula when there is not test coverage
information for a line or when the total number of failed tests is zero.
The latter case indicates that the line is not suspicious since it did not
cause any failures, therefore, zero is returned as the suspiciousness score.
However, the former case is not considered at all since AFLuent does not include
lines with no coverage information.
Fig.\ref{fig:ochiai_div_by_zero_1} and Fig.\ref{fig:ochiai_div_by_zero_2}
demonstrates that equation after plugging in
the zero values from \ref{fig:ochiaiEquation}

\begin{figure}[!htb]
	\begin{center}
        When \textbf{N$_{CF}$ = 0} and \textbf{N$_{UF}$ = 0 }
		\begin{equation}
			Ochiai(element) = \frac{\textbf{0}}{\sqrt{(\textbf{0}) \cdot (\textbf{0}  + \textbf{N$_{CS}$})}}
		\end{equation}
		\caption{\label{fig:ochiai_div_by_zero_1} Ochiai Division by Zero Example 1}
	\end{center}
\end{figure}

\begin{figure}[!htb]
	\begin{center}
        When \textbf{N$_{CF}$ = 0} and \textbf{N$_{CS}$ = 0 }
		\begin{equation}
			Ochiai(element) = \frac{\textbf{0}}{\sqrt{(\textbf{0}  + \textbf{N$_{UF}$}) \cdot (\textbf{0})}}
		\end{equation}
		\caption{\label{fig:ochiai_div_by_zero_2} Ochiai Division by Zero Example 2}
	\end{center}
\end{figure}

Fig.\ref{fig:ochiai_div_by_zero_1} is an example of when there is zero total
failed test cases, resulting in a zero suspiciousness score.
However Fig\ref{fig:ochiai_div_by_zero_2} shows an example where there is no
failed or successful tests that cover the line. This scenario does not
occur in AFLuent, which only looks at lines that have some coverage data through
passing or failing tests. Therefore, it wasn't necessary to handle this possibility.

\subsubsection{Division by Zero: DStar}
\label{subsubsec:div_by_zero_dstar}

In the DStar equation, division by zero takes place only in one case. When the
number of passing test cases that cover the line AND the number of failed test
cases that do not cover the line are both zero, the denominator evaluates to
zero. This translates to the following: if there is no passing tests executing
this line and no failing test executing other lines only, then this line should
have the maximum suspiciousness score possible. However, since DStar has a
numerator raised to a power set by the user, it has no numerical upper limit on
the resulting score. To resolve this issue, the maximum integer value is
assigned as the suspiciousness score of this line.
Fig.\ref{fig:dstar_div_by_zero} shows and example of this scenario after
plugging in values from Fig.\ref{fig:dstarEquation}

\begin{figure}[!htb]
	\begin{center}
        When \textbf{N$_{CS}$ = 0} and \textbf{N$_{UF}$ = 0 }
		\begin{equation}
			Dstar(element) = \frac{(\textbf{N$_{CF}$})^{\ast}}{\textbf{0}}
		\end{equation}
		\caption{\label{fig:dstar_div_by_zero} DStar Division by Zero Example}
	\end{center}
\end{figure}

\subsection{ProjFile Object}
\label{subsec:projfile_obj}

ProjFile objects are designed to contain attributes that describe whole files.
Additionally, they support functionality that apply to these files. The most
important attributes of these objects is the \code{lines} instance variable,
which stores a dictionary of contents of the file. Specifically, the keys in
this dictionary are line numbers in the file and the values are the Line objects
discussed previously. In addition to storing this data, ProjFile implements an
intuitive way to update it with a new result of a test case. \code{update\_file}
function takes in a list of numbers of covered lines executed by a test case as well as the
result and the name of that test case. Following that, it iterates through the
covered line numbers and updates the Line objects stored in the ProjFile
dictionary. This approach creates a hierarchy between the objects by nesting
Line objects inside of ProjFile objects.

\subsection{Spectrum Object}
\label{subsec:spectrum_obj}

This object contains some of the core functionalities that organizes the
collected data and displays it to the console. Similar to how a ProjFile object
stores a dictionary of line number and Line object pairs, Spectrum takes nesting
a step further and stores a dictionary of file paths and ProjFile objects as
key-value pairs. Additionally, it keeps track of running totals of passed,
failed, and skipped test cases, which are used to calculate suspiciousness
scores. Lastly, it's responsible for sorting Line objects in a descending order
from highest suspiciousness scores. Once this task is completed, the Spectrum
object produces a table formatted report with color coded results that
demonstrate to the user the outcome of fault localization.

\subsection{Afluent Object}
\label{subsec:afluent_obj}

While this Object does not implement any functionality regarding the calculation
or sorting of suspiciousness scores, it plays an important role in integrating
this tool with Pytest. By containing several of the Pytest hooks mentioned
prior, this object is registered as the plugin when AFLuent is enabled through
command line arguments. Following the end of the Pytest session, the AFluent
object initializes a Spectrum Object and forwards the arguments collected from
the console command. Following that, it calls the Spectrum object to produce and
print its report. Overall, this object acts as a pipeline that passes coverage
information and command line arguments collected from the Pytest session to the
infrastructure responsible for performing fault localization.

\subsection{Objects Overview}
\label{subsec:obj_overview}

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=15.5cm]{object_oriented_structure.png}
		\caption{\label{fig:oop_structure} AFLuent Object Structure}
	\end{center}
\end{figure}

Fig\ref{fig:oop_structure} provides a visual simplification of the different
components of AFluent and an overview of their roles in the functioning of the
tool. Overall the nested structure creates several layers that facilitate
development by isolating the different components and hiding unnecessary
information from other objects in the hierarchy. By following this structures,
unit tests can be written much easier and debugging becomes a simpler task.

\section{AFLuent's Output}
\label{sec:afluent_output}

While the main purpose of AFLuent is to produce a report of the most suspicious
statements, there are many additional features that can increase the readability
and user friendliness of the report. The input and workflow of AFLuent was
discussed in the previous sections, this section focuses on the outcome and
discusses how the user should interact with it.

\subsection{Feedback Messages}
\label{subsec:feedback_messages}

Throughout the execution of AFLuent, unexpected things can occur. And while
Python and Pytest error messages can be produced, AFLuent also shows console
output that gives a hint regarding what went wrong. There are three tiers of
messages produced by AFLuent, Error, Warning, and Success messages, each color
coded using red, orange, and green respectively. Each one is designed to stand
out to the user and provide a clear and concise description of the scenario.

\subsubsection{Success Messages}
\label{subsubsec:success_message}

This message is produced in the case that the test suite passes with no
error or failures. Using bright green highlighted message with bold white
letters, the message displays: \code{All tests passed, no need to diagnose using
AFLuent}. Fig\ref{fig:success_message} demonstrates the success message output
when AFLuent is ran on the project's test suite.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=15.5cm]{success_message.png}
		\caption{\label{fig:success_message} Example of Success Message}
	\end{center}
\end{figure}


\subsubsection{Error Messages}
\label{subsubsec:error_message}

Unlike success messages, error messages are produced when things aren't going as
expected. However, in this case, they aren't meant to denote complete failure of
AFLuent, but rather to communicate a major alert to the user. This type of
message is produced when the Pytest session exists with status code 1, which
occurs when at least one test fails. Fig.\ref{fig:error_message} shows the
console output when a test failure was purposefully introduced. A bright red
highlighted message with bold white letters indicate that fault localization
using AFLuent is underway.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=15.5cm]{error_message.png}
		\caption{\label{fig:error_message} Example of Error Message}
	\end{center}
\end{figure}

\subsubsection{Warning Messages}
\label{subsubsec:warning_message}

The last type of messages produced during the runtime of AFLuent are warning
messages. They communicate important information to the user but do not signify
a major problem. Warning messages are displayed in three possible scenarios. In
the case that the user passes the \code{-x} or \code{---maxfail} command line
arguments to Pytest, the session can stop at the first test failure. This is
problematic from a fault localization standpoint because it prevents further
collection of coverage information and test outcomes from the rest of the test
suite. In order to let the user be aware of this issue, the warning message
shown in Fig.\ref{fig:warning_message_1} is displayed. Regardless, AFLuent will
continue running with standard behavior, however, results are very likely to be
inaccurate.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=15.5cm]{warning_message_1.png}
		\caption{\label{fig:warning_message_1} Maxfail Warning Message}
	\end{center}
\end{figure}

Another warning message is displayed when AFLuent is installed to the current
Python environment but not enabled by the user through the \code{---afl} or
\code{---afl-debug} flags. This message serves as a reminder to the user to
enable the plugin if they're interested in utilizing fault localization. The
message is shown in Fig\ref{fig:warning_message_2}.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=15.5cm]{warning_message_2.png}
		\caption{\label{fig:warning_message_2} AFLuent Disabled Warning Message}
	\end{center}
\end{figure}

The third and last case that causes a warning message takes place when a potential
conflict with Coverage.py is detected. In the possibility that another coverage
collecting Pytest plugin is enabled in the same session as AFLuent, coverage
data could be mixed up leading to inaccurate results or internal errors in
Pytest. To avoid that, AFLuent scans the enabled plugins for the current session
and checks if \code{pytest-cov} is running. If so, a warning message is
displayed but the session continues as expected.

\emph{TODO: add the figure showing the warning message once the feature is implemented}

\subsection{Console Report}
\label{subsec:console_output}

In the case that AFLuent was enabled for the Pytest session and at least one
failure occurred, a report is generated and printed to the console to help the
user look for the error. Based on the inputs passed in the command-line
arguments, this report can be structured in many different ways. Generally, the
report is a table with rows containing the path to the file, the line number,
and suspiciousness score(s). The number of rows depends on the value of
\code{---afl-results} argument, which is set to 20 by default. Additionally, the
user can determine one or many equations to use for suspiciousness score
calculations, where the results will be sorted using the first one mentioned in
the command. For example, the command \code{poetry run pytest --afl --afl-ignore
tests/* --ochiai --dstar --afl-results 10} will display the top 10 results
sorted by the most suspicious using the Ochiai equation. However, the
DStar suspiciousness will also be displayed. Fig.\ref{fig:report_1} shows an
example of the report when running the AFLuent test suite after introducing a
bug in the code.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=15.5cm]{report_1.png}
		\caption{\label{fig:report_1} Example Report Using Ochiai and DStar}
	\end{center}
\end{figure}

In addition to printing the report, AFLuent color codes elements based on their
rankings. The four possible colors are green, yellow, orange, and red, where
each one represents the likelihood of finding the fault at that line.
Suspiciousness scores highlighted using green signify that the corresponding
element is safe and has a zero suspiciousness score. On the opposite end of the
spectrum, scores are highlighted with red when they equal the maximum
suspiciousness score possible (usually 1), the color indicates that these
elements are extremely likely to be the ones causing the fault. For the
remaining results, the top 20\% are highlighted using orange to show that they
are risky of being faulty. The remaining non-zero elements are highlighted using
yellow. Fig.\ref{fig:report_2} shows an example of how safe statements are
displayed in the report.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width=15.5cm]{report_2.png}
		\caption{\label{fig:report_2} Example Report With Safe Statements}
	\end{center}
\end{figure}

\subsection{File Report}
\label{subsec:file_output}

Aside from the reports produced in the console, AFLuent can create report files
of the rankings in both JSON and CSV formats. Additional reporting of per-test
coverage is also possible. The command line arguments \code{–-report} and
\code{–-per-test-report} enable this feature. Producing these reports facilitates
the evaluation of AFLuent by allowing the storage of rankings in a file for
further analysis later on.

\emph{TODO: include further discussion on the file reports and their contents}

\section{Complexity Metrics and Tie Breaking}
\label{sec:complexity}

\emph{TODO: begin writing once implementation is completed}

\subsection{Cyclomatic Complexity}
\label{subsec:c_complexity}

\subsection{Syntax Complexity}
\label{subsec:s_complexity}