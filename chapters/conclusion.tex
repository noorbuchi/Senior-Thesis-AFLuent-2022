\chapter{Discussion and Future Work}
\label{ch:conclusion}

\section{Summary of Results}

The results of this study can be split up into two main contributions, as the
title suggests, implementation and evaluation. AFLuent represents a first-of-a-kind SBFL tool directly integrated to the Pytest framework. Furthermore, its
emphasis on ease of use and readability makes it a great fit for
novice developers. AFLuent as a tool is maintainable, readable, and extendable
in several ways that can potentially serve for more thorough evaluations
of SBFL algorithms in the future. In addition to implementation technical
details, an evaluation of AFLuent on two main metrics, effectiveness and
efficiency, is presented. After a visual and statistical analysis of \emph{EXAM} scores for each equation
and tie breaker pair implemented in AFLuent, much was revealed regarding the
best and worst performances. One observation that stood out
is the significant underperformance of Tarantula when compared to all other
equations. Additionally, the statistically significant edge that logical tie
breaking achieve to outperform it's random and cyclomatic counterparts was
surprising. Overall, Ochiai, Ochiai2 and DStar had very strong performances
when the same tie breaker was used across all of them, however, the data did not
suggest that one outperformed the other significantly. Additional experiments may
yield different results and uncover new information about these equations. As for the most effective tie breaking technique, the
logical approach emerged as the most successful in creating a statistically
significant difference followed by the enhanced tie breaker.
Additionally, the presented time data demonstrate the
time overhead that developers need to take on if they choose to use
AFLuent. While further evaluation is needed to determine the time efficiency of
each approach independently from others, the worst case time data suggest that
calculating per test coverage and performing fault localization will lead to a
significant increase in runtime.

\section{Future Work}
\label{sec:future_work}

Extensions can be added to AFLuent to explore more approaches in the
SBFL field as well as increase the tool's ability to find faults. These
types of additional work can be split up to three main sub-categories:
effectiveness, efficiency, and evaluation.

\subsubsection{Effectiveness}

Many SBFL equations have been studied and evaluated in previous literature,
however, AFLuent's inclusion of only the four, Tarantula, Ochiai, Ochiai2, and
DStar is only the first step in analyzing how fault localization can perform in
Python projects. Additionally, setting a limit is needed due to time
constraints. Possible extensions of this work can implement more SBFL equation
options for developers to utilize, which could be especially helpful if new
techniques prove to be more effective than those currently implemented. Another
feature of AFLuent that significantly influenced the collected performance
metrics is the approach used for tie breaking. Additional enhancements and
techniques can be added to improve existing ones, especially the `enhanced' tie
breaker. This could be completed by expanding the list of possible mutants and
improving the analysis of the abstract syntax trees. These potential improvements
can be easily added to the existing code by implementing new functions to calculate suspiciousness
or break ties in the object oriented structure. This extension would allow developers
to add their favorite SBFL equation into AFLuent easily.

\subsubsection{Efficiency}

One of the concerning outcomes of this study is the long time AFLuent takes to
produce fault localization output. Developer usually want fast and optimized
tools, which could render AFLuent unusable in the eyes of many. And while
AFLuent's reliance on several tools reduce the ability to control the time it
takes to run, there are some measures that could mitigate this problem.
Throughout manual testing of the tool, it was generally observed that the most
time consuming feature of AFLuent involves generating the tie breaker datasets.
In oder for AFLuent to adequately understand the code under test, it must
generate abstract syntax tree for every file covered in the test suite. Since
the debugging process usually involves making small changes at a time and
rerunning the tests, a time improvement is possible be caching all generated
syntax trees and only re-generating ones for files that have been edited since
the last run. This solution will not reduce the runtime for the first time but
it could have a significant effect on the runs that follow. Overall, this change
introduces a quality of life improvement by reducing the time developers need to
wait before receiving the report from AFLuent. Once these improvements have been
made, a more detailed evaluation of AFLuent's performance is also needed. By
comparing each equation and tie breaking approach combination independently, a
better understanding can be reached regarding how AFLuent performs based on the
used approach.

\subsubsection{Evaluation}

While the evaluation section of this study managed to filter out many
ineffective combinations of equations and tie breakers, it could not reach a
definite conclusion on the best approach to perform SBFL. Furthermore, the
evaluation was conducted on a small scale by focusing on small projects with low
complexity. Time and feasibility constraints were very prevalent, which led to
adopting a strategy to simulate novice written code and unit tests rather than
including human subjects to write, test, and debug the code. Even though
the used strategy has some flaws, it is justifiable considering the scope of this
study. To improve on
the current understanding of AFLuent's performance,
additional experiments are needed on a larger sample of Python projects that
includes a variety of programming styles and utilizes a diverse set of Python
constructs. Furthermore, extending the
study to include novice developers by asking them to write and debug their code
using AFLuent would provide great insight on their perspective on the tool
and how they plan to use it. For example, a potentially more thorough evaluation of AFLuent
would consist of these steps:
\begin{enumerate}
    \item Select computer science students with various levels of experience
    \item Assign the students to write and test a collection of programs
    \begin{itemize}
        \item Students can be split into groups to complete different assignments
        \item Chosen assignments should allow for test driven development
    \end{itemize}
    \item Allow some groups of students to use AFLuent and assess it's
    effectiveness in helping them locate faults
    \item Collect direct feedback regarding the students' experience while using AFLuent
\end{enumerate}
While these steps do not provide a comprehensive plan for an alternative
approach of evaluating AFLuent, it seeks to take into consideration the human
experience aspect of using and interacting with the tool. This metric is missing
from the current evaluation, and AFLuent, as well as the SBFL field, could
benefit from more data surrounding it.

\section{Ethical Implications}

Aside from answering research questions in the form of implementation and
evaluation, this study includes an interesting ethical question regarding the
potential misuse of a tool such as AFLuent. One major difference between AFLuent
and a traditional debugger is the way a developer interacts with each. In order
to use a traditional debugger, the developer must be well versed in the code
base and understand where breakpoints should be placed. Additionally, they must
be able to identify what suspicious behavior looks like. On the other hand,
AFLuent simplifies this process and provides a statement ranking for the
developer to parse through and check. By doing so, AFLuent serves its goal in
making the debugging process simpler and more efficient, but it could lead to
over reliance, especially from novice developers. If developers become too
dependent on AFLuent, or any automated fault localization tool, it could
negatively impact their development skills by taking away the experience of
manually analyzing the code and understanding its expected behavior and why
failures occur. This could be especially harmful if AFLuent was overused in
educational environments since students could utilize it's functionality without
fully understanding how to fix the code themselves, and thus negatively impact
their learning process.

